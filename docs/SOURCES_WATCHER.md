# Sources CSV Watcher & Auto-Scraper

Automatisches Scraping-System, das bei Ã„nderungen an `_data/sources.csv` sofort ein Event-Scraping startet.

## ğŸ¯ Zweck

Nach dem Bearbeiten der `sources.csv` (Quellen hinzufÃ¼gen/entfernen/aktivieren) wird automatisch:
1. Die Ã„nderung erkannt
2. Die aktualisierten Quellen geladen
3. Ein kompletter Scraping-Durchlauf gestartet
4. Eine Zusammenfassung angezeigt

## ğŸš€ Verwendung

### Option 1: Watcher-Modus (empfohlen)

Startet einen Hintergrund-Prozess, der `sources.csv` Ã¼berwacht:

```bash
# Bash-Script (prÃ¼ft AbhÃ¤ngigkeiten automatisch)
./scripts/scrape.sh

# Oder direkt Python
python3 scripts/sources_watcher.py --watch
```

**Output:**
```
================================================================================
ğŸ” SOURCES.CSV WATCHER
================================================================================
Ãœberwache: _data/sources.csv
Scraper:   scripts/editorial/scrape_events.py
Logs:      _events/_logs
================================================================================

ğŸ’¡ Bearbeite sources.csv um automatisch Scraping zu starten
ğŸ›‘ DrÃ¼cke Ctrl+C zum Beenden
```

Sobald du `sources.csv` speicherst:
```
[18:45:12] [INFO] ğŸ“ sources.csv wurde geÃ¤ndert!
[18:45:12] [INFO] ğŸ“Š Aktive Quellen: 6
[18:45:12] [INFO]    1. Stadt Hof (html)
[18:45:12] [INFO]    2. Freiheitshalle Hof (html)
[18:45:12] [INFO]    3. Galeriehaus Hof (facebook)
[18:45:12] [INFO] ğŸš€ Starte Scraping mit aktualisierten Quellen...
[18:45:15] [SUCCESS] âœ… Scraping erfolgreich abgeschlossen!
[18:45:15] [INFO] ğŸ“„ Log-Datei: 20251119-184512-scraping.log

================================================================================
SCRAPING-ZUSAMMENFASSUNG
================================================================================
ğŸ“Š ZUSAMMENFASSUNG
   âœ“ Quellen gescannt: 2
   âœ“ Events gefunden: 5
   âœ“ Neue Events: 3
   âœ“ Duplikate: 2
   âœ“ Fehler: 0
================================================================================
```

### Option 2: Manuelles Triggern

Einmaliges Scraping ohne Watcher:

```bash
./scripts/scrape.sh --trigger

# Oder
python3 scripts/sources_watcher.py --trigger
```

### Option 3: Klassisches Scraping (ohne Watcher)

```bash
python3 scripts/editorial/scrape_events.py
```

## ğŸ“‹ Voraussetzungen

### Python-Paket: watchdog

```bash
# Installation
pip install watchdog

# Oder aus requirements.txt
pip install -r requirements.txt
```

Das Bash-Script `scripts/scrape.sh` prÃ¼ft automatisch ob `watchdog` installiert ist und bietet Installation an.

## ğŸ”§ Funktionsweise

### 1. File Watcher (Watchdog)
```python
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class SourcesChangeHandler(FileSystemEventHandler):
    def on_modified(self, event):
        if event.src_path.endswith('sources.csv'):
            # Hash prÃ¼fen (nur echte Ã„nderungen)
            # Debounce (max. 1x alle 2 Sekunden)
            # Scraping triggern
```

### 2. Ã„nderungs-Erkennung
- **Hash-Vergleich**: MD5-Hash von `sources.csv` wird verglichen
- **Debounce**: Mehrfache Saves innerhalb 2 Sekunden = 1x Scraping
- **Nur echte Ã„nderungen**: Speichern ohne Ã„nderung triggert nichts

### 3. Auto-Scraping
```python
subprocess.run([sys.executable, "scripts/editorial/scrape_events.py"])
```

- FÃ¼hrt `scrape_events.py` aus
- Timeout: 5 Minuten
- Zeigt Zusammenfassung aus Log-Datei

## ğŸ“ Dateien

```
scripts/
  sources_watcher.py    # Python Watcher & Auto-Scraper
  scrape_events.py      # Haupt-Scraping-Script

scripts/scrape.sh        # Bash Convenience-Script

_data/
  sources.csv           # Event-Quellen (Ã¼berwacht)

_events/
  _logs/
    YYYYMMDD-HHMMSS-scraping.log  # Scraping-Logs
```

## ğŸ›ï¸ Konfiguration

### Debounce-Zeit anpassen

In `sources_watcher.py`:
```python
self.debounce_seconds = 2  # Standard: 2 Sekunden
```

### Timeout anpassen

In `sources_watcher.py`:
```python
result = subprocess.run(
    ...,
    timeout=300  # Standard: 5 Minuten
)
```

## ğŸ§ª Beispiel-Workflow

### 1. Watcher starten
```bash
./scripts/scrape.sh
```

### 2. Sources bearbeiten
Ã–ffne `_data/sources.csv`:
```csv
name,url,type,active,notes
Stadt Hof,https://www.hof.de/...,html,true,Offizielle Seite
Neue Quelle,https://example.com,html,true,Neue Event-Quelle  # â† NEU
```

### 3. Speichern â†’ Automatisches Scraping
```
[18:50:00] [INFO] ğŸ“ sources.csv wurde geÃ¤ndert!
[18:50:00] [INFO] ğŸ“Š Aktive Quellen: 7
[18:50:00] [INFO]    7. Neue Quelle (html)
[18:50:00] [INFO] ğŸš€ Starte Scraping mit aktualisierten Quellen...
```

### 4. Ergebnis prÃ¼fen
```bash
# Neueste Log-Datei
ls -t _events/_logs/*-scraping.log | head -1

# Neue Events
ls -t _events/*.md | head -5
```

## ğŸ” Troubleshooting

### "watchdog nicht installiert"
```bash
pip install watchdog
```

### "Scraping-Timeout"
- Scraper lÃ¤uft lÃ¤nger als 5 Minuten
- ErhÃ¶he `timeout` in `sources_watcher.py`
- Oder prÃ¼fe warum Scraper hÃ¤ngt

### "Keine Ã„nderung erkannt"
- Hash ist identisch (keine echte Ã„nderung)
- Debounce aktiv (zu schnell gespeichert)
- Datei nicht in `_data/` (Watcher Ã¼berwacht nur diesen Ordner)

### "Scraping schlÃ¤gt fehl"
```bash
# Direkter Test ohne Watcher
python3 scripts/editorial/scrape_events.py

# Log-Datei prÃ¼fen
cat _events/_logs/$(ls -t _events/_logs/*-scraping.log | head -1)
```

## ğŸ¯ Use Cases

### 1. Neue Event-Quelle hinzufÃ¼gen
```csv
Neue Venue,https://venue.com/events,html,true,Beschreibung
```
â†’ Speichern â†’ Automatisches Scraping â†’ Events in `_events/`

### 2. Quelle temporÃ¤r deaktivieren
```csv
Alte Quelle,https://...,html,false,Inaktiv
```
â†’ Speichern â†’ Scraping ohne diese Quelle

### 3. Mehrere Quellen gleichzeitig Ã¤ndern
- Alle Ã„nderungen machen
- Einmal speichern
- Ein Scraping-Durchlauf mit allen Ã„nderungen

## ğŸš€ Integration in Workflow

### VS Code Tasks (tasks.json)
```json
{
  "label": "Watch Sources & Auto-Scrape",
  "type": "shell",
  "command": "./scripts/scrape.sh",
  "isBackground": true,
  "problemMatcher": []
}
```

### Development Script
```bash
# In dev.sh
echo "Starte Sources Watcher..."
./scripts/scrape.sh &
WATCHER_PID=$!

# Jekyll starten
bundle exec jekyll serve

# Cleanup
kill $WATCHER_PID
```

## ğŸ“Š Features

âœ… **Automatische Erkennung**: Speichern von `sources.csv` triggert Scraping  
âœ… **Debounce**: Verhindert mehrfaches Scraping bei schnellen Edits  
âœ… **Hash-Vergleich**: Nur echte Ã„nderungen triggern Scraping  
âœ… **Live-Feedback**: Zeigt Scraping-Progress in Echtzeit  
âœ… **Zusammenfassung**: Log-Output direkt in Console  
âœ… **Timeout-Protection**: Max. 5 Minuten pro Scraping  
âœ… **Error-Handling**: Fehler werden sauber geloggt  
âœ… **Manueller Modus**: Auch einmaliges Triggern mÃ¶glich  

## ğŸ”„ NÃ¤chste Schritte

1. **VS Code Extension**: Sources-Editor mit Live-Preview
2. **Web-UI**: Browser-basierter Sources-Manager
3. **Scheduled Scraping**: Cron-Job + Watcher kombinieren
4. **Multi-Source-Scraping**: Paralleles Scraping mehrerer Quellen
5. **Scraping-Queue**: Ã„nderungen sammeln, batch-verarbeiten
